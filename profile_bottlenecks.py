"""
Profile time bottlenecks in LearnableGaussianField implementation.
"""

import torch
from torch import nn
from torch.nn import functional as F
import time
import numpy as np
from line_profiler import LineProfiler


class LearnableGaussianField(nn.Module):
    """Copy of the class for profiling."""
    
    def __init__(self, num_gaussians: int, volume_size: float = 10.0, use_full_cov: bool = True):
        super().__init__()
        
        self.num_gaussians = num_gaussians
        self.volume_size = volume_size
        self.use_full_cov = use_full_cov
        
        # Initialize learnable parameters
        scale = volume_size / np.cbrt(num_gaussians)
        
        # Gaussian centers: uniformly distributed in volume
        self.means = nn.Parameter(torch.rand(num_gaussians, 3) * volume_size)
        
        if use_full_cov:
            init_scale = np.log(scale)
            self.cov_tril = nn.Parameter(torch.tensor([
                [init_scale, 0.0, init_scale, 0.0, 0.0, init_scale]
            ]).repeat(num_gaussians, 1))
        else:
            self.log_scales = nn.Parameter(torch.ones(num_gaussians, 3) * np.log(scale))
        
        # Weights (amplitudes)
        self.weights = nn.Parameter(torch.ones(num_gaussians))
    
    def get_covariance(self) -> torch.Tensor:
        """Reconstruct full covariance matrices from Cholesky parameters."""
        if not self.use_full_cov:
            # Diagonal covariance
            scales = torch.exp(self.log_scales)  # [N, 3]
            cov = torch.zeros(self.num_gaussians, 3, 3, device=scales.device)
            cov[:, 0, 0] = scales[:, 0] ** 2
            cov[:, 1, 1] = scales[:, 1] ** 2
            cov[:, 2, 2] = scales[:, 2] ** 2
            return cov
        
        # Construct lower triangular matrices from 6 parameters
        L = torch.zeros(self.num_gaussians, 3, 3, device=self.cov_tril.device)
        
        # Diagonal elements (always positive via exp)
        L[:, 0, 0] = torch.exp(self.cov_tril[:, 0])
        L[:, 1, 1] = torch.exp(self.cov_tril[:, 2])
        L[:, 2, 2] = torch.exp(self.cov_tril[:, 5])
        
        # Off-diagonal elements
        L[:, 1, 0] = self.cov_tril[:, 1]
        L[:, 2, 0] = self.cov_tril[:, 3]
        L[:, 2, 1] = self.cov_tril[:, 4]
        
        # Compute covariance: Î£ = L @ L^T
        cov = torch.bmm(L, L.transpose(-2, -1))  # [N, 3, 3]
        
        # Add small regularization for numerical stability
        cov = cov + 1e-6 * torch.eye(3, device=cov.device).unsqueeze(0)
        
        return cov
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Evaluate implicit function at point x with full covariance support."""
        # Handle batched input
        if x.dim() == 1:
            x = x.unsqueeze(0)
            squeeze_output = True
        else:
            squeeze_output = False
        
        B = x.shape[0]
        
        # Compute differences: [B, N, 3]
        diff = x.unsqueeze(1) - self.means.unsqueeze(0)
        
        # Get covariance matrices
        cov = self.get_covariance()  # [N, 3, 3]
        
        # Compute Mahalanobis distance using solve for numerical stability
        mahal = torch.zeros(B, self.num_gaussians, device=x.device)
        
        for i in range(self.num_gaussians):
            diff_i = diff[:, i, :]
            v = torch.linalg.solve(cov[i].unsqueeze(0).expand(B, -1, -1), 
                                   diff_i.unsqueeze(-1))
            v = v.squeeze(-1)
            mahal[:, i] = (diff_i * v).sum(dim=-1)
        
        # Weighted sum of Gaussians
        gaussians = torch.exp(-0.5 * mahal)
        output = (gaussians * self.weights.unsqueeze(0)).sum(dim=-1)
        
        return output.squeeze(0) if squeeze_output else output


def benchmark_forward_pass():
    """Benchmark the forward pass with different configurations."""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print("=" * 80)
    print("BENCHMARK: Forward Pass")
    print(f"Device: {device}")
    print("=" * 80)
    
    configs = [
        (100, 500, True),   # 100 Gaussians, 500 points, full cov
        (100, 500, False),  # 100 Gaussians, 500 points, diagonal
        (1000, 500, True),  # 1000 Gaussians, 500 points, full cov
        (1000, 500, False), # 1000 Gaussians, 500 points, diagonal
        (100, 5000, True),  # 100 Gaussians, 5000 points, full cov
    ]
    
    for num_g, num_pts, use_full in configs:
        model = LearnableGaussianField(num_gaussians=num_g, volume_size=10.0, use_full_cov=use_full).to(device)
        coords = torch.rand(num_pts, 3, device=device) * 10.0
        
        # Warmup
        _ = model(coords)
        
        # Benchmark
        if device == 'cuda':
            torch.cuda.synchronize()
        start = time.time()
        n_runs = 10
        for _ in range(n_runs):
            _ = model(coords)
        if device == 'cuda':
            torch.cuda.synchronize()
        elapsed = (time.time() - start) / n_runs
        
        cov_type = "Full" if use_full else "Diag"
        print(f"N={num_g:4d}, Pts={num_pts:5d}, Cov={cov_type:4s}: {elapsed*1000:.2f} ms/iter")


def benchmark_backward_pass():
    """Benchmark backward pass (gradient computation)."""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print("\n" + "=" * 80)
    print("BENCHMARK: Backward Pass (with gradients)")
    print(f"Device: {device}")
    print("=" * 80)
    
    configs = [
        (100, 500, True),
        (100, 500, False),
        (1000, 500, True),
        (1000, 500, False),
    ]
    
    for num_g, num_pts, use_full in configs:
        model = LearnableGaussianField(num_gaussians=num_g, volume_size=10.0, use_full_cov=use_full).to(device)
        coords = torch.rand(num_pts, 3, device=device) * 10.0
        targets = torch.rand(num_pts, device=device)
        
        # Warmup
        pred = model(coords)
        loss = F.mse_loss(pred, targets)
        loss.backward()
        
        # Benchmark
        if device == 'cuda':
            torch.cuda.synchronize()
        start = time.time()
        n_runs = 10
        for _ in range(n_runs):
            model.zero_grad()
            pred = model(coords)
            loss = F.mse_loss(pred, targets)
            loss.backward()
        if device == 'cuda':
            torch.cuda.synchronize()
        elapsed = (time.time() - start) / n_runs
        
        cov_type = "Full" if use_full else "Diag"
        print(f"N={num_g:4d}, Pts={num_pts:5d}, Cov={cov_type:4s}: {elapsed*1000:.2f} ms/iter (fwd+bwd)")


def profile_forward_detailed():
    """Detailed profiling of forward pass components."""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print("\n" + "=" * 80)
    print("DETAILED PROFILING: Forward Pass Components")
    print(f"Device: {device}")
    print("=" * 80)
    
    model = LearnableGaussianField(num_gaussians=1000, volume_size=10.0, use_full_cov=True).to(device)
    coords = torch.rand(1000, 3, device=device) * 10.0
    
    with torch.no_grad():
        B = coords.shape[0]
        
        # 1. Difference computation
        if device == 'cuda':
            torch.cuda.synchronize()
        start = time.time()
        for _ in range(100):
            diff = coords.unsqueeze(1) - model.means.unsqueeze(0)
        if device == 'cuda':
            torch.cuda.synchronize()
        t_diff = (time.time() - start) / 100
        
        # 2. Covariance reconstruction
        if device == 'cuda':
            torch.cuda.synchronize()
        start = time.time()
        for _ in range(100):
            cov = model.get_covariance()
        if device == 'cuda':
            torch.cuda.synchronize()
        t_cov = (time.time() - start) / 100
        
        # 3. Mahalanobis distance (the bottleneck)
        cov = model.get_covariance()
        diff = coords.unsqueeze(1) - model.means.unsqueeze(0)
        
        if device == 'cuda':
            torch.cuda.synchronize()
        start = time.time()
        for _ in range(100):
            mahal = torch.zeros(B, model.num_gaussians, device=device)
            for i in range(model.num_gaussians):
                diff_i = diff[:, i, :]
                v = torch.linalg.solve(cov[i].unsqueeze(0).expand(B, -1, -1), 
                                       diff_i.unsqueeze(-1))
                v = v.squeeze(-1)
                mahal[:, i] = (diff_i * v).sum(dim=-1)
        if device == 'cuda':
            torch.cuda.synchronize()
        t_mahal = (time.time() - start) / 100
        
        # 4. Gaussian computation and weighting
        if device == 'cuda':
            torch.cuda.synchronize()
        start = time.time()
        for _ in range(100):
            gaussians = torch.exp(-0.5 * mahal)
            output = (gaussians * model.weights.unsqueeze(0)).sum(dim=-1)
        if device == 'cuda':
            torch.cuda.synchronize()
        t_final = (time.time() - start) / 100
    
    print(f"\nComponent breakdown (N=1000, B=1000, Full Covariance):")
    print(f"  1. Difference computation:     {t_diff*1000:.2f} ms ({t_diff/(t_diff+t_cov+t_mahal+t_final)*100:.1f}%)")
    print(f"  2. Covariance reconstruction:  {t_cov*1000:.2f} ms ({t_cov/(t_diff+t_cov+t_mahal+t_final)*100:.1f}%)")
    print(f"  3. Mahalanobis distance loop:  {t_mahal*1000:.2f} ms ({t_mahal/(t_diff+t_cov+t_mahal+t_final)*100:.1f}%)")
    print(f"  4. Gaussian + weighting:       {t_final*1000:.2f} ms ({t_final/(t_diff+t_cov+t_mahal+t_final)*100:.1f}%)")
    print(f"  TOTAL:                         {(t_diff+t_cov+t_mahal+t_final)*1000:.2f} ms")


def optimized_forward(model, x):
    """Vectorized version without loop - faster alternative."""
    if x.dim() == 1:
        x = x.unsqueeze(0)
        squeeze_output = True
    else:
        squeeze_output = False
    
    B = x.shape[0]
    N = model.num_gaussians
    
    # Compute differences: [B, N, 3]
    diff = x.unsqueeze(1) - model.means.unsqueeze(0)
    
    # Get covariance matrices
    cov = model.get_covariance()  # [N, 3, 3]
    
    # Vectorized Mahalanobis distance computation
    # Expand cov: [N, 3, 3] -> [B, N, 3, 3]
    cov_expanded = cov.unsqueeze(0).expand(B, -1, -1, -1)
    
    # Solve for all at once: [B, N, 3, 1]
    v = torch.linalg.solve(cov_expanded, diff.unsqueeze(-1)).squeeze(-1)
    
    # Compute Mahalanobis: [B, N]
    mahal = (diff * v).sum(dim=-1)
    
    # Weighted sum of Gaussians
    gaussians = torch.exp(-0.5 * mahal)
    output = (gaussians * model.weights.unsqueeze(0)).sum(dim=-1)
    
    return output.squeeze(0) if squeeze_output else output


def benchmark_optimized_version():
    """Compare original vs optimized forward pass."""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print("\n" + "=" * 80)
    print("OPTIMIZATION COMPARISON: Original vs Vectorized")
    print(f"Device: {device}")
    print("=" * 80)
    
    model = LearnableGaussianField(num_gaussians=1000, volume_size=10.0, use_full_cov=True).to(device)
    coords = torch.rand(1000, 3, device=device) * 10.0
    
    # Original version
    if device == 'cuda':
        torch.cuda.synchronize()
    start = time.time()
    n_runs = 20
    for _ in range(n_runs):
        _ = model(coords)
    if device == 'cuda':
        torch.cuda.synchronize()
    t_original = (time.time() - start) / n_runs
    
    # Optimized version
    if device == 'cuda':
        torch.cuda.synchronize()
    start = time.time()
    for _ in range(n_runs):
        _ = optimized_forward(model, coords)
    if device == 'cuda':
        torch.cuda.synchronize()
    t_optimized = (time.time() - start) / n_runs
    
    speedup = t_original / t_optimized
    
    print(f"\nN=1000, Pts=1000, Full Covariance:")
    print(f"  Original (loop):    {t_original*1000:.2f} ms")
    print(f"  Optimized (vector): {t_optimized*1000:.2f} ms")
    print(f"  Speedup:            {speedup:.2f}x")


if __name__ == "__main__":
    benchmark_forward_pass()
    benchmark_backward_pass()
    profile_forward_detailed()
    benchmark_optimized_version()
    
    print("\n" + "=" * 80)
    print("KEY FINDINGS:")
    print("=" * 80)
    print("1. Mahalanobis distance loop is the main bottleneck (~90% of time)")
    print("2. Full covariance is ~2-3x slower than diagonal")
    print("3. Vectorized solve() is much faster than looping over Gaussians")
    print("4. Recommendation: Replace loop with vectorized operations")
